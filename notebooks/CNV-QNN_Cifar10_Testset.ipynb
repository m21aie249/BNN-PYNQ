{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cifar-10 testset classification on Pynq\n",
    "\n",
    "This notebook covers how to use low quantized neural networks on Pynq. \n",
    "It shows an example how CIFAR-10 testset can be inferred utilizing different precision neural networks inspired at VGG-16, featuring 6 convolutional layers, 3 max pool layers and 3 fully connected layers. There are 3 different precision available:\n",
    "\n",
    "- CNVW1A1 using 1 bit weights and 1 bit activation,\n",
    "- CNVW1A2 using 1 bit weights and 2 bit activation and\n",
    "- CNVW2A2 using 2 bit weights and 2 bit activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import the package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Cifar-10 testset\n",
    "\n",
    "This notebook required the testset from https://www.cs.toronto.edu/~kriz/cifar.html which contains 10000 images that can be processed by CNV network directly without preprocessing.\n",
    "\n",
    "You can download the cifar-10 set from given url via wget and unzip it to a folder on Pynq as shown below.\n",
    "This may take a while as the training set is included in the archive as well.\n",
    "After that we need to read the labels from the binary file to be able to compare the results later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-04-14 10:46:32--  https://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz\n",
      "Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30\n",
      "Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 170052171 (162M) [application/x-gzip]\n",
      "Saving to: ‘cifar-10-binary.tar.gz’\n",
      "\n",
      "cifar-10-binary.tar 100%[===================>] 162.17M   770KB/s    in 4m 18s  \n",
      "\n",
      "2023-04-14 10:50:52 (643 KB/s) - ‘cifar-10-binary.tar.gz’ saved [170052171/170052171]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#get\n",
    "!wget https://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz\n",
    "#unzip\n",
    "!tar -xf cifar-10-binary.tar.gz\n",
    "\n",
    "labels = []\n",
    "with open(\"/home/xilinx/jupyter_notebooks/bnn/cifar-10-batches-bin/test_batch.bin\", \"rb\") as file:\n",
    "    #for 10000 pictures\n",
    "    for i in range(10000):\n",
    "        #read first byte -> label\n",
    "        labels.append(int.from_bytes(file.read(1), byteorder=\"big\"))\n",
    "        #read image (3072 bytes) and do nothing with it\n",
    "        file.read(3072)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Start inference\n",
    "\n",
    "The inference can be performed with different precision for weights and activation. Creating a specific Classifier will automatically download the correct bitstream onto PL and load the weights and thresholds trained on the specific dataset. \n",
    "\n",
    "Thus that images are already Cifar-10 preformatted no preprocessing is required. Therefor the functions `classify_cifar` or `classify_cifars` can be used. When classifying non Cifar-10 formatted pictures refer to `classify_image` or `classify_images`  (see Notebook CNV-QNN_Cifar10).\n",
    "\n",
    "### Case 1: \n",
    "#### W1A1 - 1 bit weight and 1 activation\n",
    "\n",
    "Instantiate the classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "try {\n",
       "require(['notebook/js/codecell'], function(codecell) {\n",
       "  codecell.CodeCell.options_default.highlight_modes[\n",
       "      'magic_text/x-csrc'] = {'reg':[/^%%microblaze/]};\n",
       "  Jupyter.notebook.events.one('kernel_ready.Kernel', function(){\n",
       "      Jupyter.notebook.get_cells().map(function(cell){\n",
       "          if (cell.cell_type == 'code'){ cell.auto_highlight(); } }) ;\n",
       "  });\n",
       "});\n",
       "} catch (e) {};\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "try {\n",
       "require(['notebook/js/codecell'], function(codecell) {\n",
       "  codecell.CodeCell.options_default.highlight_modes[\n",
       "      'magic_text/x-csrc'] = {'reg':[/^%%pybind11/]};\n",
       "  Jupyter.notebook.events.one('kernel_ready.Kernel', function(){\n",
       "      Jupyter.notebook.get_cells().map(function(cell){\n",
       "          if (cell.cell_type == 'code'){ cell.auto_highlight(); } }) ;\n",
       "  });\n",
       "});\n",
       "} catch (e) {};\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting network weights and thresholds in accelerator...\n"
     ]
    }
   ],
   "source": [
    "hw_classifier = bnn.CnvClassifier(bnn.NETWORK_CNVW1A1,'cifar10',bnn.RUNTIME_HW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And start the inference on Cifar-10 preformatted multiple images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packing and interleaving CIFAR-10 inputs...\n",
      "Running prebuilt CIFAR-10 test for 10000 images...\n",
      "Inference took 3278719 microseconds, 327.872 usec per image\n",
      "Classification rate: 3049.97 images per second\n",
      "Inference took 3278718.87 microseconds, 327.87 usec per image\n",
      "Classification rate: 3049.97 images per second\n"
     ]
    }
   ],
   "source": [
    "result_W1A1 = hw_classifier.classify_cifars(\"/home/xilinx/jupyter_notebooks/bnn/cifar-10-batches-bin/test_batch.bin\")\n",
    "time_W1A1 = hw_classifier.usecPerImage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 2:\n",
    "#### W1A2 - 1 bit weight and 2 activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting network weights and thresholds in accelerator...\n"
     ]
    }
   ],
   "source": [
    "hw_classifier = bnn.CnvClassifier(bnn.NETWORK_CNVW1A2,'cifar10',bnn.RUNTIME_HW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packing and interleaving CIFAR-10 inputs...\n",
      "Running prebuilt CIFAR-10 test for 10000 images...\n",
      "Inference took 3278534 microseconds, 327.853 usec per image\n",
      "Classification rate: 3050.14 images per second\n",
      "Inference took 3278533.94 microseconds, 327.85 usec per image\n",
      "Classification rate: 3050.14 images per second\n"
     ]
    }
   ],
   "source": [
    "result_W1A2 = hw_classifier.classify_cifars(\"/home/xilinx/jupyter_notebooks/bnn/cifar-10-batches-bin/test_batch.bin\")\n",
    "time_W1A2 = hw_classifier.usecPerImage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 3:\n",
    "#### W2A2 - 2 bit weight and 2 activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting network weights and thresholds in accelerator...\n"
     ]
    }
   ],
   "source": [
    "hw_classifier = bnn.CnvClassifier(bnn.NETWORK_CNVW2A2,'cifar10',bnn.RUNTIME_HW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packing and interleaving CIFAR-10 inputs...\n",
      "Running prebuilt CIFAR-10 test for 10000 images...\n",
      "Inference took 11626631 microseconds, 1162.66 usec per image\n",
      "Classification rate: 860.094 images per second\n",
      "Inference took 11626630.86 microseconds, 1162.66 usec per image\n",
      "Classification rate: 860.09 images per second\n"
     ]
    }
   ],
   "source": [
    "result_W2A2 = hw_classifier.classify_cifars(\"/home/xilinx/jupyter_notebooks/bnn/cifar-10-batches-bin/test_batch.bin\")\n",
    "time_W2A2 = hw_classifier.usecPerImage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference time\n",
    "\n",
    "Results can be visualized using `matplotlib`. Here the comparison of hardware execution time is plotted in microseconds per Image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "height = [time_W1A1, time_W1A2, time_W2A2]\n",
    "bars   = ('W1A1', 'W1A2', 'W2A2')\n",
    "\n",
    "y_pos=range(3)\n",
    "plt.bar(y_pos, height, 0.5)\n",
    "plt.xticks(y_pos, bars)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "\n",
    "The accuracy on the testset can be calculated by comparing the inferred labels against the one read at the beginning:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy W1A1:  79.22 %\n",
      "Accuracy W1A2:  82.66 %\n",
      "Accuracy W2A2:  84.29 %\n"
     ]
    }
   ],
   "source": [
    "#compare against labels\n",
    "countRight = 0\n",
    "for idx in range(len(labels)):\n",
    "    if labels[idx] == result_W1A1[idx]:\n",
    "        countRight += 1\n",
    "accuracyW1A1 = countRight*100/len(labels)\n",
    "\n",
    "countRight = 0\n",
    "for idx in range(len(labels)):\n",
    "    if labels[idx] == result_W1A2[idx]:\n",
    "        countRight += 1\n",
    "accuracyW1A2 = countRight*100/len(labels)\n",
    "\n",
    "countRight = 0\n",
    "for idx in range(len(labels)):\n",
    "    if labels[idx] == result_W2A2[idx]:\n",
    "        countRight += 1\n",
    "accuracyW2A2 = countRight*100/len(labels)\n",
    "\n",
    "print(\"Accuracy W1A1: \",accuracyW1A1,\"%\")\n",
    "print(\"Accuracy W1A2: \",accuracyW1A2,\"%\")\n",
    "print(\"Accuracy W2A2: \",accuracyW2A2,\"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
